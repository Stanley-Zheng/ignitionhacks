{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hORD6yoVY2W3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "outputId": "6119102b-014a-468c-e099-159bd98d8195"
      },
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "import urllib.request\n",
        "!pip install transformers\n",
        "!pip install tokenizers\n",
        "from transformers import *\n",
        "import tokenizers\n",
        "\n",
        "# Google enviornment\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# While we're here, we might as well check what GPU we have (note that I am using Colab Pro):\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ6XdbUUrdaK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ceb5558-2889-42b9-b70f-e9fa7a5398b7"
      },
      "source": [
        "# I would normally do data visualization, but I'm short on time today. Begin data wrangling! :)\n",
        "\n",
        "# Next, we load our CSV's using pandas\n",
        "train = pd.read_csv('/content/gdrive/My Drive/ignitionhacks/training_data.csv')\n",
        "val = pd.read_csv ('/content/gdrive/My Drive/ignitionhacks/val.csv')\n",
        "\n",
        "# Now we can preprocess our data (making sure that we do the same to test and train). \n",
        "\n",
        "from transformers import AutoTokenizer, TFAutoModelWithLMHead\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"jplu/tf-xlm-roberta-base\")\n",
        "\n",
        "trainenc=[]\n",
        "trainattn=[]\n",
        "\n",
        "valenc=[]\n",
        "valattn=[]\n",
        "\n",
        "# First, we encode the text from the panda's dataframe we loaded earlier\n",
        "for i in train.Text:\n",
        "  enc = tokenizer.encode(i)\n",
        "  trainenc.append(enc)\n",
        "\n",
        "# Keras has some quick and easy preprocessing that I don't have time to write a function for, it works fine (and converts to a tensor)\n",
        "trainenc = tf.keras.preprocessing.sequence.pad_sequences(trainenc, maxlen=256, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Now we make an attention mask\n",
        "for i in trainenc:\n",
        "  att=[int(x > 0) for x in i]\n",
        "  trainattn.append(att)\n",
        "\n",
        "# Finally, we just put all of our training and validation data into a tf.dataset so it loads faster (from experience, Colab I/O is painfully slow)\n",
        "train = tf.data.Dataset.from_tensor_slices((trainenc, trainattn, train.Sentiment))\n",
        "\n",
        "# Same thing for our validation data\n",
        "for i in val.Text:\n",
        "  enc = tokenizer.encode(i)\n",
        "  valenc.append(enc)  \n",
        "\n",
        "valenc = tf.keras.preprocessing.sequence.pad_sequences(valenc, maxlen=256, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "for i in valenc:\n",
        "  att=[int(x > 0) for x in i]\n",
        "  valattn.append(att)\n",
        "\n",
        "val = tf.data.Dataset.from_tensor_slices((valenc, valattn, val.Sentiment))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNZlj5BQ71Sj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finally (though it didn't take *that* long), we are ready to train. \n",
        "\n",
        "class roBERTaClassifier(tf.keras.Model):    \n",
        "    def __init__(self, bert: TFBertModel, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.classifier = tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
        "        \n",
        "    @tf.function\n",
        "    def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids,\n",
        "                               attention_mask=attention_mask,\n",
        "                               token_type_ids=token_type_ids,\n",
        "                               position_ids=position_ids,\n",
        "                               head_mask=head_mask)\n",
        "        cls_output = outputs[1]\n",
        "        cls_output = self.classifier(cls_output)\n",
        "                \n",
        "        return cls_output\n",
        "\n",
        "model = roBERTaClassifier(TFBertModel.from_pretrained(\"jplu/tf-xlm-roberta-base\"), 1)\n",
        "\n",
        "# We can define some metrics to better evaluate our model\n",
        "metrics = [\n",
        "    tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
        "    tf.keras.metrics.Precision(name=\"precision\"),\n",
        "    tf.keras.metrics.AUC(name=\"auc\")\n",
        "]\n",
        "\n",
        "# Finally, we can train after compiling our model with some hyperparameters. Again, we will be using Keras since it's quick and easy. \n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-7)\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = metrics)\n",
        "!mkdir /content/gdrive/My\\ Drive/ignitionhacks/best\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='/content/gdrive/My Drive/ignitionhacks/best',\n",
        "    save_weights_only=False,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "batch_size=1024\n",
        "\n",
        "model.fit(train, validation_data=val, epochs=55, batch_size=batch_size, steps_per_epoch=len(train)//batch_size, callbacks=model_checkpoint_callback) # Yes I know, massive batch size. We're definitely reducing our accuracy with such a massive batch size, but the dataset's just... huge and time is limited.\n",
        "!mkdir /content/gdrive/My\\ Drive/ignitionhacks/final\n",
        "model.save('/content/gdrive/My Drive/ignitionhacks/')\n",
        "#model.fit((trainenc, trainattn), train.Sentiment, validation_data=((valenc,valattn), val.Sentiment), batch_size=32, epochs=150)\n",
        "# Hyperparameters were tuned using intuition\n",
        "\n",
        "# Now we play the waiting game."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fgwXzFs7KbX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "476a520b-6d10-4050-efaf-f78848b2d5f2"
      },
      "source": [
        "# Finally, we can do inference based on contestant_judgement (private test data)\n",
        "test_csv = pd.read_csv('/content/gdrive/My Drive/ignitionhacks/contestant_judgment.csv')\n",
        "\n",
        "# We can copy paste all of our preprocessing from before\n",
        "testenc=[]\n",
        "testattn=[]\n",
        "\n",
        "for i in test_csv.Text:\n",
        "  enc = tokenizer.encode(i)\n",
        "  testenc.append(enc)\n",
        "\n",
        "testenc = tf.keras.preprocessing.sequence.pad_sequences(testenc, maxlen=128, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "for i in testenc:\n",
        "  att=[int(x > 0) for x in i]\n",
        "  testattn.append(att)\n",
        "\n",
        "test = tf.data.Dataset.from_tensor_slices((testenc, testattn))\n",
        "\n",
        "print('encoded')\n",
        "\n",
        "# Make predictions using the train dataset\n",
        "predictions = model.predict(test)\n",
        "\n",
        "print('predicted')\n",
        "\n",
        "# Create a new column in our dataset for our predictions\n",
        "test_csv['Sentiment'] = predictions\n",
        "\n",
        "# Save to csv for final submission!\n",
        "test_csv.to_csv('pseudo.csv')\n",
        "!mkdir /content/gdrive/My\\ Drive/ignitionhacks/\n",
        "!cp submission.csv /content/gdrive/My\\ Drive/ignitionhacks/\n",
        "\n",
        "print('done')\n",
        "\n",
        "# Sidenote: I'm not sure why, but this takes by far the longest? Odd."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsfBq6Ar-bCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Next, we can pseudolabel on these submissions. I understand this is an advanced technique not many people use, so feel free to PM me if you have any questions.\n",
        "plabel = pd.read_csv('pseudo.csv') \n",
        "\n",
        "plabelenc=[]\n",
        "plabelattn=[]\n",
        "\n",
        "for i in plabel.Text:\n",
        "  enc = tokenizer.encode(i)\n",
        "  plabelenc.append(enc)  \n",
        "\n",
        "plabelenc = tf.keras.preprocessing.sequence.pad_sequences(plabelenc, maxlen=256, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "for i in plabelenc:\n",
        "  att=[int(x > 0) for x in i]\n",
        "  plabelattn.append(att)\n",
        "\n",
        "plabel = tf.data.Dataset.from_tensor_slices((plabelenc, plabelattn, plabel.Sentiment))\n",
        "\n",
        "model.fit(plabel, epochs=7, batch_size=batch_size, steps_per_epoch=len(plabel)//batch_size, callbacks=model_checkpoint_callback) # We don't need validation since there are so few epochs, it's not going to overfit (or at least that's the theory!). \n",
        "\n",
        "test_csv = pd.read_csv('/content/gdrive/My Drive/ignitionhacks/contestant_judgment.csv')\n",
        "\n",
        "testenc=[]\n",
        "testattn=[]\n",
        "\n",
        "for i in test_csv.Text:\n",
        "  enc = tokenizer.encode(i)\n",
        "  testenc.append(enc)\n",
        "\n",
        "testenc = tf.keras.preprocessing.sequence.pad_sequences(testenc, maxlen=128, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "for i in testenc:\n",
        "  att=[int(x > 0) for x in i]\n",
        "  testattn.append(att)\n",
        "\n",
        "test = tf.data.Dataset.from_tensor_slices((testenc, testattn))\n",
        "\n",
        "print('encoded')\n",
        "\n",
        "# Make predictions using the train dataset\n",
        "predictions = model.predict(test)\n",
        "\n",
        "print('predicted')\n",
        "\n",
        "# Create a new column in our dataset for our predictions\n",
        "test_csv['Sentiment'] = predictions\n",
        "\n",
        "# Save to csv for final submission!\n",
        "test_csv.to_csv('submission.csv')\n",
        "!mkdir /content/gdrive/My\\ Drive/ignitionhacks/\n",
        "!cp submission.csv /content/gdrive/My\\ Drive/ignitionhacks/\n",
        "\n",
        "print('done')\n",
        "\n",
        "# And we are DONE!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHGytWQm-ZGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}